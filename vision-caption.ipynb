{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n!pip install protobuf==4.25.3\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tqdm\nimport nltk\nimport os\n\nfrom tensorflow.keras.applications.xception import Xception, preprocess_input\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Dropout, add\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom nltk.translate.bleu_score import corpus_bleu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-21T17:22:00.991157Z","iopub.execute_input":"2025-11-21T17:22:00.991562Z","iopub.status.idle":"2025-11-21T17:22:36.210349Z","shell.execute_reply.started":"2025-11-21T17:22:00.991533Z","shell.execute_reply":"2025-11-21T17:22:36.208254Z"}},"outputs":[{"name":"stdout","text":"Collecting protobuf==4.25.3\n  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\nDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 6.33.0\n    Uninstalling protobuf-6.33.0:\n      Successfully uninstalled protobuf-6.33.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-4.25.3\n","output_type":"stream"},{"name":"stderr","text":"2025-11-21 17:22:12.588221: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763745732.952688      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763745733.042264      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"captions_path = \"/kaggle/input/flickr8k/captions.txt\"\nimages_path = \"/kaggle/input/flickr8k/Images\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T17:22:36.212379Z","iopub.execute_input":"2025-11-21T17:22:36.212982Z","iopub.status.idle":"2025-11-21T17:22:36.217705Z","shell.execute_reply.started":"2025-11-21T17:22:36.212954Z","shell.execute_reply":"2025-11-21T17:22:36.216781Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"num_images = len(os.listdir(images_path))\nprint(\"Total images:\", num_images)\n\nnum_captions = sum(1 for _ in open(captions_path, 'r')) - 1 # -1 for the header line\nprint(\"Total captions:\", num_captions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T17:22:36.218581Z","iopub.execute_input":"2025-11-21T17:22:36.218830Z","iopub.status.idle":"2025-11-21T17:22:36.415455Z","shell.execute_reply.started":"2025-11-21T17:22:36.218811Z","shell.execute_reply":"2025-11-21T17:22:36.413144Z"}},"outputs":[{"name":"stdout","text":"Total images: 8091\nTotal captions: 40455\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def load_captions(filepath):\n    captions = {}\n    with open(filepath, 'r') as f:\n        next(f, None)  # skip header if it exists\n        for line in f:\n            \n            if ',' in line:\n                parts = line.split(',', 1)\n\n            image_id, caption = parts[0], parts[1].strip()\n\n            if image_id not in captions:\n                captions[image_id] = []\n\n            captions[image_id].append(caption)\n    return captions\n\n\ncaptions = load_captions(captions_path)\nprint(\"Unique image IDs:\", len(captions))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T17:22:36.417463Z","iopub.execute_input":"2025-11-21T17:22:36.418245Z","iopub.status.idle":"2025-11-21T17:22:36.456970Z","shell.execute_reply.started":"2025-11-21T17:22:36.418210Z","shell.execute_reply":"2025-11-21T17:22:36.455889Z"}},"outputs":[{"name":"stdout","text":"Unique image IDs: 8091\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"for i, (img_id, caption_list) in enumerate(captions.items()):\n    for caption in caption_list:\n        print(img_id, \":\", caption)\n    if i == 2:   # stop after printing 3 image IDs (0, 1, 2)\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T17:22:36.458001Z","iopub.execute_input":"2025-11-21T17:22:36.458295Z","iopub.status.idle":"2025-11-21T17:22:36.463780Z","shell.execute_reply.started":"2025-11-21T17:22:36.458271Z","shell.execute_reply":"2025-11-21T17:22:36.462934Z"}},"outputs":[{"name":"stdout","text":"1000268201_693b08cb0e.jpg : A child in a pink dress is climbing up a set of stairs in an entry way .\n1000268201_693b08cb0e.jpg : A girl going into a wooden building .\n1000268201_693b08cb0e.jpg : A little girl climbing into a wooden playhouse .\n1000268201_693b08cb0e.jpg : A little girl climbing the stairs to her playhouse .\n1000268201_693b08cb0e.jpg : A little girl in a pink dress going into a wooden cabin .\n1001773457_577c3a7d70.jpg : A black dog and a spotted dog are fighting\n1001773457_577c3a7d70.jpg : A black dog and a tri-colored dog playing with each other on the road .\n1001773457_577c3a7d70.jpg : A black dog and a white dog with brown spots are staring at each other in the street .\n1001773457_577c3a7d70.jpg : Two dogs of different breeds looking at each other on the road .\n1001773457_577c3a7d70.jpg : Two dogs on pavement moving toward each other .\n1002674143_1b742ab4b8.jpg : A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl .\n1002674143_1b742ab4b8.jpg : A little girl is sitting in front of a large painted rainbow .\n1002674143_1b742ab4b8.jpg : A small girl in the grass plays with fingerpaints in front of a white canvas with a rainbow on it .\n1002674143_1b742ab4b8.jpg : There is a girl with pigtails sitting in front of a rainbow painting .\n1002674143_1b742ab4b8.jpg : Young girl with pigtails painting outside in the grass .\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import string\n\ndef clean_captions(captions_dict):\n    table = str.maketrans('', '', string.punctuation)\n\n    # maketrans(x, y, z) means:\n    # x → characters to replace\n    # y → characters to replace them with\n    # z → characters to delete\n    \n    cleaned = {}\n    for img_id, caption_list in captions_dict.items():\n        cleaned[img_id] = []\n        for caption in caption_list:\n            caption = caption.lower().translate(table)\n            caption = caption.strip()\n            caption = ' '.join([word for word in caption.split() if len(word) > 1])\n            caption = 'startseq ' + caption + ' endseq'\n            cleaned[img_id].append(caption)\n    return cleaned\n\ncaptions_cleaned = clean_captions(captions)\nprint(\"Example:\", captions_cleaned[list(captions_cleaned.keys())[0]][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T17:22:36.464776Z","iopub.execute_input":"2025-11-21T17:22:36.464968Z","iopub.status.idle":"2025-11-21T17:22:36.605879Z","shell.execute_reply.started":"2025-11-21T17:22:36.464953Z","shell.execute_reply":"2025-11-21T17:22:36.604785Z"}},"outputs":[{"name":"stdout","text":"Example: startseq child in pink dress is climbing up set of stairs in an entry way endseq\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from tensorflow.keras.applications.xception import Xception, preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\nmodel_xcep = Xception(include_top=False, pooling='avg')\nfeatures = {}\n\nfor img_id in tqdm.tqdm(captions_cleaned.keys()):\n    img_path = os.path.join(images_path, img_id)\n    img = load_img(img_path, target_size=(299, 299))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = preprocess_input(img)\n    feature = model_xcep.predict(img, verbose=0)\n    features[img_id] = feature[0]\n\nprint(\"Extracted features:\", len(features))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T17:22:36.607794Z","iopub.execute_input":"2025-11-21T17:22:36.608097Z","iopub.status.idle":"2025-11-21T18:02:13.868098Z","shell.execute_reply.started":"2025-11-21T17:22:36.608072Z","shell.execute_reply":"2025-11-21T18:02:13.866863Z"}},"outputs":[{"name":"stderr","text":"2025-11-21 17:22:36.619787: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 8091/8091 [39:35<00:00,  3.41it/s]","output_type":"stream"},{"name":"stdout","text":"Extracted features: 8091\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# ⚠️ Save this as a .pkl file once done so you don’t re-run it again (it’s slow)","metadata":{}},{"cell_type":"code","source":"import pickle\npickle.dump(features, open(\"/kaggle/working/xception_features.pkl\", \"wb\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T18:02:13.869003Z","iopub.execute_input":"2025-11-21T18:02:13.869250Z","iopub.status.idle":"2025-11-21T18:02:14.073899Z","shell.execute_reply.started":"2025-11-21T18:02:13.869227Z","shell.execute_reply":"2025-11-21T18:02:14.072682Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import pickle\nfeatures = pickle.load(open(\"/kaggle/working/xception_features.pkl\", \"rb\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T18:02:14.074977Z","iopub.execute_input":"2025-11-21T18:02:14.075287Z","iopub.status.idle":"2025-11-21T18:02:14.162099Z","shell.execute_reply.started":"2025-11-21T18:02:14.075267Z","shell.execute_reply":"2025-11-21T18:02:14.161077Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"all_captions = []\nfor key in captions_cleaned:\n    all_captions.extend(captions_cleaned[key])\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(all_captions)\nvocab_size = len(tokenizer.word_index) + 1\n\nmax_length = max(len(caption.split()) for caption in all_captions)\nprint(\"Vocab size:\", vocab_size)\nprint(\"Max caption length:\", max_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T18:02:14.166771Z","iopub.execute_input":"2025-11-21T18:02:14.167008Z","iopub.status.idle":"2025-11-21T18:02:14.547283Z","shell.execute_reply.started":"2025-11-21T18:02:14.166993Z","shell.execute_reply":"2025-11-21T18:02:14.545506Z"}},"outputs":[{"name":"stdout","text":"Vocab size: 8811\nMax caption length: 34\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"pickle.dump(tokenizer, open(\"/kaggle/working/tokenizer.pkl\", \"wb\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T18:02:14.548566Z","iopub.execute_input":"2025-11-21T18:02:14.548933Z","iopub.status.idle":"2025-11-21T18:02:14.565370Z","shell.execute_reply.started":"2025-11-21T18:02:14.548905Z","shell.execute_reply":"2025-11-21T18:02:14.564083Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"tokenizer = pickle.load(open(\"tokenizer.pkl\", \"rb\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T18:02:14.566269Z","iopub.execute_input":"2025-11-21T18:02:14.566495Z","iopub.status.idle":"2025-11-21T18:02:14.585016Z","shell.execute_reply.started":"2025-11-21T18:02:14.566477Z","shell.execute_reply":"2025-11-21T18:02:14.583237Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def create_sequences(tokenizer, max_length, captions_list, image_feature, vocab_size):\n    X1, X2, y = [], [], []\n    for caption in captions_list:\n        seq = tokenizer.texts_to_sequences([caption])[0]\n        for i in range(1, len(seq)):\n            in_seq, out_seq = seq[:i], seq[i]\n            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n            out_seq = tf.keras.utils.to_categorical([out_seq], num_classes=vocab_size)[0]\n            X1.append(image_feature)\n            X2.append(in_seq)\n            y.append(out_seq)\n    return np.array(X1), np.array(X2), np.array(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T18:02:14.586329Z","iopub.execute_input":"2025-11-21T18:02:14.586619Z","iopub.status.idle":"2025-11-21T18:02:14.614820Z","shell.execute_reply.started":"2025-11-21T18:02:14.586599Z","shell.execute_reply":"2025-11-21T18:02:14.613648Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# feature extractor (encoder)\ninputs1 = Input(shape=(2048,))\nfe1 = Dropout(0.5)(inputs1)\nfe2 = Dense(256, activation='relu')(fe1)\n\n# sequence processor (decoder)\ninputs2 = Input(shape=(max_length,))\nse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\nse2 = Dropout(0.5)(se1)\nse3 = LSTM(256)(se2)\n\n# decoder (merge both)\ndecoder1 = add([fe2, se3])\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\n\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T18:02:14.615806Z","iopub.execute_input":"2025-11-21T18:02:14.616038Z","iopub.status.idle":"2025-11-21T18:02:14.812994Z","shell.execute_reply.started":"2025-11-21T18:02:14.616020Z","shell.execute_reply":"2025-11-21T18:02:14.811150Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m2,255,616\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m524,544\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m525,312\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│                     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_12 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                     │                   │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8811\u001b[0m)      │  \u001b[38;5;34m2,264,427\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,255,616</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│                     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                     │                   │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8811</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,264,427</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,635,691\u001b[0m (21.50 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,635,691</span> (21.50 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,635,691\u001b[0m (21.50 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,635,691</span> (21.50 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"sample_image_ids = list(captions_cleaned.keys())[:250]  # first 250 images\nX1, X2, y = [], [], []\n\nfor img_id in sample_image_ids:\n    # flatten feature (1, 2048) → (2048,)\n    feature_vec = features[img_id]\n    if feature_vec.ndim == 2:   # only if shape is (1, 2048)\n        feature_vec = feature_vec.reshape(2048,)\n\n    x1, x2, y_ = create_sequences(\n        tokenizer, max_length, captions_cleaned[img_id], feature_vec, vocab_size\n    )\n    X1.append(x1)\n    X2.append(x2)\n    y.append(y_)\n\nX1 = np.vstack(X1)\nX2 = np.vstack(X2)\ny = np.vstack(y)\n\nmodel.fit([X1, X2], y, epochs=20, batch_size=64, verbose=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T18:02:14.814856Z","iopub.execute_input":"2025-11-21T18:02:14.815236Z","iopub.status.idle":"2025-11-21T18:15:46.805033Z","shell.execute_reply.started":"2025-11-21T18:02:14.815218Z","shell.execute_reply":"2025-11-21T18:15:46.802954Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n203/203 - 44s - 215ms/step - loss: 5.8250\nEpoch 2/20\n203/203 - 40s - 195ms/step - loss: 4.6730\nEpoch 3/20\n203/203 - 38s - 188ms/step - loss: 3.9885\nEpoch 4/20\n203/203 - 39s - 191ms/step - loss: 3.4355\nEpoch 5/20\n203/203 - 40s - 197ms/step - loss: 2.9846\nEpoch 6/20\n203/203 - 40s - 199ms/step - loss: 2.5951\nEpoch 7/20\n203/203 - 40s - 198ms/step - loss: 2.2885\nEpoch 8/20\n203/203 - 43s - 210ms/step - loss: 2.0399\nEpoch 9/20\n203/203 - 41s - 203ms/step - loss: 1.8422\nEpoch 10/20\n203/203 - 43s - 211ms/step - loss: 1.6802\nEpoch 11/20\n203/203 - 42s - 209ms/step - loss: 1.5356\nEpoch 12/20\n203/203 - 41s - 200ms/step - loss: 1.4003\nEpoch 13/20\n203/203 - 39s - 190ms/step - loss: 1.3034\nEpoch 14/20\n203/203 - 42s - 208ms/step - loss: 1.1927\nEpoch 15/20\n203/203 - 40s - 195ms/step - loss: 1.1007\nEpoch 16/20\n203/203 - 38s - 189ms/step - loss: 1.0124\nEpoch 17/20\n203/203 - 38s - 189ms/step - loss: 0.9314\nEpoch 18/20\n203/203 - 39s - 190ms/step - loss: 0.8661\nEpoch 19/20\n203/203 - 41s - 203ms/step - loss: 0.8004\nEpoch 20/20\n203/203 - 43s - 209ms/step - loss: 0.7434\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7e637da185d0>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n\n# generate caption (inference function)\ndef generate_caption(model, tokenizer, photo, max_length):\n\n    photo = np.expand_dims(photo, axis=0)\n    in_text = 'startseq'\n    for _ in range(max_length):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        yhat = np.argmax(model.predict([photo, sequence], verbose=0))\n        word = next((w for w, i in tokenizer.word_index.items() if i == yhat), None)\n        if word is None: break\n        in_text += ' ' + word\n        if word == 'endseq': break\n    return in_text\n\n# example\nimg_id = list(features.keys())[0]\nphoto = features[img_id]\ncaption = generate_caption(model, tokenizer, photo, max_length)\nprint(\"Generated caption:\", caption)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T18:19:15.578906Z","iopub.execute_input":"2025-11-21T18:19:15.579163Z","iopub.status.idle":"2025-11-21T18:19:16.391024Z","shell.execute_reply.started":"2025-11-21T18:19:15.579148Z","shell.execute_reply":"2025-11-21T18:19:16.390084Z"}},"outputs":[{"name":"stdout","text":"Generated caption: startseq little girl climbing into wooden building endseq\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}